{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ==========================================================\n",
    "#### üìò RAG-BASED AUTO GRADER \n",
    "#### ==========================================================\n",
    "#### Requirements:\n",
    "#### pip install sentence-transformers openai PyPDF2 numpy pandas tiktoken tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PyPDF2\n",
    "# !pip install sentence-transformers openai numpy pandas tqdm genai anthropic \n",
    "# ! pip install PyPDF2 python-docx python-pptx pandas sentence-transformers tqdm openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deva/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, re, json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PyPDF2 import PdfReader\n",
    "from docx import Document\n",
    "from pptx import Presentation\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import openai\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \".././Srinivasan/data/\"\n",
    "CHUNK_SIZE = 550\n",
    "CHUNK_OVERLAP = 50\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- CONFIG -------------\n",
    "\n",
    "\n",
    "\n",
    "embedder = SentenceTransformer(EMBED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Active LLM provider: openai\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# üåê CLIENT INITIALIZATION\n",
    "# ==========================================================\n",
    "if ACTIVE_LLM_PROVIDER == \"openai\":\n",
    "    # openai is already imported in cell 2\n",
    "    openai.api_key = OPENAI_API_KEY\n",
    "    def ask_llm(prompt, model=\"gpt-4o-mini\", temperature=0.2):\n",
    "        resp = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"system\", \"content\": \"Return STRICT JSON only.\"},\n",
    "                      {\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return resp[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "elif ACTIVE_LLM_PROVIDER == \"gemini\":\n",
    "    import google.generativeai as genai\n",
    "    genai.configure(api_key=GOOGLE_API_KEY)\n",
    "    def ask_llm(prompt, model=\"gemini-2.0-flash-lite\", temperature=0.2):\n",
    "        model = genai.GenerativeModel(model)\n",
    "        resp = model.generate_content(prompt)\n",
    "        return resp.text.strip()\n",
    "\n",
    "elif ACTIVE_LLM_PROVIDER == \"claude\":\n",
    "    import anthropic\n",
    "    client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "    def ask_llm(prompt, model=\"claude-3-5-sonnet-20240620\", temperature=0.2):\n",
    "        msg = client.messages.create(\n",
    "            model=model,\n",
    "            max_tokens=1024,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return msg.content[0].text.strip()\n",
    "\n",
    "elif ACTIVE_LLM_PROVIDER == \"llama\":\n",
    "    from huggingface_hub import InferenceClient\n",
    "    client = InferenceClient(token=HUGGINGFACE_TOKEN)\n",
    "\n",
    "    def ask_llm(prompt, model=\"meta-llama/Meta-Llama-3-8B-Instruct\", temperature=0.2):\n",
    "        \"\"\"\n",
    "        Uses conversational mode since Llama models are registered under that task.\n",
    "        \"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Return STRICT JSON only.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        resp = client.chat_completion(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            max_tokens=1024,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        return resp[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "\n",
    "elif ACTIVE_LLM_PROVIDER == \"copilot\":\n",
    "    # Example for Azure OpenAI / GitHub Copilot-like API\n",
    "    import openai\n",
    "    openai.api_type = \"azure\"\n",
    "    openai.api_key = AZURE_API_KEY\n",
    "    openai.api_base = \"https://your-azure-endpoint.openai.azure.com\"\n",
    "    openai.api_version = \"2024-03-01-preview\"\n",
    "    def ask_llm(prompt, model=\"gpt-4\", temperature=0.2):\n",
    "        resp = openai.ChatCompletion.create(\n",
    "            engine=model,\n",
    "            messages=[{\"role\": \"system\", \"content\": \"Return STRICT JSON only.\"},\n",
    "                      {\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return resp[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported provider: {ACTIVE_LLM_PROVIDER}\")\n",
    "\n",
    "print(f\"‚úÖ Active LLM provider: {ACTIVE_LLM_PROVIDER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- GENERIC TEXT EXTRACTORS ----------\n",
    "def extract_text_from_pdf(path):\n",
    "    reader = PdfReader(path)\n",
    "    return \"\\n\".join([p.extract_text() or \"\" for p in reader.pages])\n",
    "\n",
    "def extract_text_from_docx(path):\n",
    "    doc = Document(path)\n",
    "    return \"\\n\".join([p.text for p in doc.paragraphs if p.text.strip()])\n",
    "\n",
    "def extract_text_from_pptx(path):\n",
    "    prs = Presentation(path)\n",
    "    text = []\n",
    "    for s in prs.slides:\n",
    "        for sh in s.shapes:\n",
    "            if hasattr(sh, \"text\"):\n",
    "                text.append(sh.text)\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "def extract_text_from_csv(path):\n",
    "    df = pd.read_csv(path)\n",
    "    return \" \".join(df.astype(str).fillna(\"\").values.flatten())\n",
    "\n",
    "def extract_text_from_txt(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- UNIVERSAL LOADER ----------\n",
    "def extract_text_from_any(path):\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext == \".pdf\":\n",
    "        return extract_text_from_pdf(path)\n",
    "    elif ext == \".csv\":\n",
    "        return extract_text_from_csv(path)\n",
    "    elif ext == \".docx\":\n",
    "        return extract_text_from_docx(path)\n",
    "    elif ext == \".pptx\":\n",
    "        return extract_text_from_pptx(path)\n",
    "    elif ext == \".txt\":\n",
    "        return extract_text_from_txt(path)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Unsupported file type: {path}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- CHUNKING -----------------------------\n",
    "def chunk_text(text, chunk_size=550, overlap=50):\n",
    "    tokens = re.split(r'\\s+', text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), chunk_size - overlap):\n",
    "        chunk = \" \".join(tokens[i:i + chunk_size])\n",
    "        if chunk.strip():\n",
    "            chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(path, chunk_size=550, overlap=50):\n",
    "    all_chunks = []\n",
    "\n",
    "    if os.path.isfile(path):\n",
    "        # Handle single file\n",
    "        text = extract_text_from_any(path)\n",
    "        chunks = chunk_text(text, chunk_size, overlap)\n",
    "        for c in chunks:\n",
    "            all_chunks.append({\"source\": os.path.basename(path), \"content\": c})\n",
    "        return all_chunks\n",
    "\n",
    "    elif os.path.isdir(path):\n",
    "        # Handle folder with multiple files\n",
    "        for file in os.listdir(path):\n",
    "            fpath = os.path.join(path, file)\n",
    "            if os.path.isfile(fpath):\n",
    "                text = extract_text_from_any(fpath)\n",
    "                chunks = chunk_text(text, chunk_size, overlap)\n",
    "                for c in chunks:\n",
    "                    all_chunks.append({\"source\": file, \"content\": c})\n",
    "        return all_chunks\n",
    "\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Invalid path: {path}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- LOAD RUBRIC -----------------------\n",
    "\n",
    "\n",
    "def load_rubric_text(path=\"./data/rubric.csv\"):\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext == \".csv\":\n",
    "        df = pd.read_csv(path).fillna(\"\")\n",
    "        return \"\\n\".join([\n",
    "            f\"{r.Criterion} | {r.Weight} | {r.Level} | {r.Description}\"\n",
    "            for r in df.itertuples()\n",
    "        ])\n",
    "    return extract_text_from_any(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# RETRIEVAL\n",
    "# ==========================================================\n",
    "notes_docs = load_documents(os.path.join(DATA_PATH, \"notes.pdf\"))\n",
    "rubric_text = load_rubric_text(os.path.join(DATA_PATH, \"rubric.csv\"))\n",
    "all_texts = [d[\"content\"] for d in notes_docs]\n",
    "embeddings = embedder.encode(all_texts, convert_to_tensor=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- RETRIEVAL ----------------------------\n",
    "def retrieve_context(query, top_k=3):\n",
    "    q_emb = embedder.encode(query, convert_to_tensor=True)\n",
    "    hits = util.semantic_search(q_emb, embeddings, top_k=top_k)[0]\n",
    "    return [notes_docs[h[\"corpus_id\"]] for h in hits]\n",
    "\n",
    "def prepare_indexed_context(retrieved):\n",
    "    return \"\\n\\n\".join([f\"[R{i}] {r['content']}\" for i, r in enumerate(retrieved, 1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# COVERAGE CHECK (GROUNDING)\n",
    "# ==========================================================\n",
    "def split_sentences(txt):\n",
    "    return [s.strip() for s in re.split(r'(?<=[.!?])\\s+', txt) if s.strip()]\n",
    "\n",
    "def coverage_check(answer, retrieved, sim_threshold=0.5):\n",
    "    sents = split_sentences(answer)\n",
    "    if not sents or not retrieved:\n",
    "        return 0.0, sents\n",
    "    retr_texts = [r[\"content\"] for r in retrieved]\n",
    "    retr_emb = embedder.encode(retr_texts, convert_to_tensor=True)\n",
    "    sent_emb = embedder.encode(sents, convert_to_tensor=True)\n",
    "    unsupported = []\n",
    "    for i, e in enumerate(sent_emb):\n",
    "        max_sim = float(util.cos_sim(e, retr_emb).max().cpu().item())\n",
    "        if max_sim < sim_threshold:\n",
    "            unsupported.append(sents[i])\n",
    "    ratio = 0.0 if not sents else (len(sents)-len(unsupported))/len(sents)\n",
    "    return ratio, unsupported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- PROMPT BUILDER -----------------------\n",
    "# ==========================================================\n",
    "# PROMPT (STRICTLY GROUNDED + STRICTNESS CONTROL)\n",
    "# ==========================================================\n",
    "def build_prompt(question, answer, context, rubric_text, max_score=50, strictness=3):\n",
    "    \"\"\"\n",
    "    Builds a balanced, notes-grounded grading prompt.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    return f\"\"\"\n",
    "You are an academic auto-grader evaluating a student's reflection response.\n",
    "\n",
    "\n",
    "### REFERENCE MATERIAL (from course notes)\n",
    "Use this content as the authoritative source when grading. Only grade ideas that are supported by this material:\n",
    "{context}\n",
    "\n",
    "### INSTRUCTOR RUBRIC (from file)\n",
    "Use this rubric exactly as written. The criteria, weights, and levels define how grading should be done:\n",
    "{rubric_text}\n",
    "\n",
    "### QUESTION\n",
    "{question}\n",
    "\n",
    "### STUDENT ANSWER\n",
    "{answer}\n",
    "\n",
    "---\n",
    "\n",
    "### YOUR TASK\n",
    "Grade the student's answer based **only** on the above notes and rubric.\n",
    "\n",
    "1. **Relevance:** If the student's content does not appear in the notes, treat it as off-topic or unsupported, based on the strictness level.\n",
    "2. **Per Criterion Evaluation:**\n",
    "   - Identify the rubric criterion name.\n",
    "   - Assign a \"score\" between 0‚Äì5.\n",
    "   - Write a short \"comments\" paragraph (1‚Äì2 sentences) specific to the topic, explaining what was good or missing.\n",
    "   - Avoid generic phrases like ‚Äúgood job‚Äù or ‚Äúneeds more detail.‚Äù Instead, reference the actual content (e.g., Dakota, land, sacred power, geography, identity, etc.).\n",
    "3. **Overall Feedback:**\n",
    "   - Compute the overall grade according to the rubric‚Äôs weights and levels.\n",
    "   - Provide a short \"feedback_summary\" (2‚Äì3 sentences) summarizing performance.\n",
    "   - Mention specific areas of strength or improvement related to the question.\n",
    "4. **Unsupported Content:**\n",
    "   - List exact sentences or ideas from the student‚Äôs answer that are **not supported by the notes**.\n",
    "5. **Correct Answer Retrieval:**\n",
    "   - From the provided notes, find and quote or paraphrase the most relevant passage that represents the correct answer. \n",
    "   - Include its source name or page number if visible.\n",
    "\n",
    "---\n",
    "\n",
    "### OUTPUT FORMAT (STRICT JSON ONLY)\n",
    "Return only a valid JSON object in this structure:\n",
    "\n",
    "{{\n",
    "  \"criteria\": [\n",
    "    {{\n",
    "      \"criterion\": \"<criterion name from rubric>\",\n",
    "      \"score\": 4,\n",
    "      \"comments\": \"Shows clear understanding of the Dakota concept of sacred geography, but lacks examples from the notes.\"\n",
    "    }},\n",
    "    {{\n",
    "      \"criterion\": \"<criterion name from rubric>\",\n",
    "      \"score\": 5,\n",
    "      \"comments\": \"Well written, clear, and supported by the course material.\"\n",
    "    }}\n",
    "  ],\n",
    "  \"unsupported_claims\": [\n",
    "    \"The Dakota people worshipped in temples.\"\n",
    "  ],\n",
    "  \"final_score\": 0,\n",
    "  \"max_score\": {max_score},\n",
    "  \"feedback_summary\": \"Good comprehension and structure, though some claims are not supported by notes.\",\n",
    "  \"correct_answer\": {{\n",
    "    \"source\": \"notes.pdf page 3\",\n",
    "    \"content\": \"The 'sacred power of place' refers to the Dakota belief that land itself is sacred and embodies memory and identity...\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "### RULES\n",
    "- Output **JSON only** ‚Äî no markdown, no explanations.\n",
    "- Follow the **strictness level** when deciding leniency or harshness.\n",
    "- Always include `\"criterion\"`, `\"score\"`, `\"comments\"` for each rubric section.\n",
    "- Always provide `\"correct_answer\"` with source and content.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- SAFE JSON PARSER -----------------------\n",
    "\n",
    "import json, re\n",
    "\n",
    "def safe_json_parse(raw_output: str):\n",
    "    \"\"\"\n",
    "    Safely parse possibly malformed JSON output from an LLM.\n",
    "    Tries multiple cleaning strategies automatically.\n",
    "    \"\"\"\n",
    "    if not raw_output or not raw_output.strip():\n",
    "        raise ValueError(\"Empty response from model ‚Äî no JSON returned.\")\n",
    "\n",
    "    candidates = [raw_output]\n",
    "\n",
    "    # remove markdown fences\n",
    "    candidates.append(re.sub(r\"```(json)?\", \"\", raw_output).strip())\n",
    "\n",
    "    # replace single quotes with double quotes cautiously\n",
    "    candidates.append(re.sub(r\"'\", '\"', raw_output))\n",
    "\n",
    "    # remove trailing commas\n",
    "    candidates.append(re.sub(r\",\\s*([}\\]])\", r\"\\1\", raw_output))\n",
    "\n",
    "    for text in candidates:\n",
    "        try:\n",
    "            return json.loads(text)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "    # final fallback: extract possible JSON substring\n",
    "    match = re.search(r\"\\{.*\\}\", raw_output, re.S)\n",
    "    if match:\n",
    "        try:\n",
    "            return json.loads(match.group(0))\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "    print(\"‚ö†Ô∏è Could not parse model JSON output:\\n\", raw_output[:600])\n",
    "    raise json.JSONDecodeError(\"LLM output not valid JSON\", raw_output, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "myPrompt = \"\"\n",
    "\n",
    "def grade_answer(question, answer, max_score=50, top_k=3, sim_threshold=0.5, strictness=3):\n",
    "    \"\"\"\n",
    "    Notes-grounded grading (balanced):\n",
    "    - Grades student answer using rubric + note context.\n",
    "    - Lists unsupported claims.\n",
    "    - Returns the best-matched 'correct answer' snippet from notes.\n",
    "    - Keeps JSON clean and human-readable.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Retrieve note chunks related to question ---\n",
    "    query = f\"{question} {answer}\"\n",
    "    retrieved = retrieve_context(query, top_k=top_k)\n",
    "    context = prepare_indexed_context(retrieved)\n",
    "\n",
    "    # --- Build prompt for the LLM ---\n",
    "    prompt = build_prompt(\n",
    "        question=question,\n",
    "        answer=answer,\n",
    "        context=context,\n",
    "        rubric_text=rubric_text,\n",
    "        max_score=max_score,\n",
    "        strictness = strictness\n",
    "    )\n",
    "    myPrompt = prompt\n",
    "\n",
    "    # --- Ask the LLM to grade ---\n",
    "    raw_output = ask_llm(prompt)\n",
    "\n",
    "    # --- Parse JSON safely ---\n",
    "    data = safe_json_parse(raw_output)\n",
    "\n",
    "    # --- Normalize per-criterion fields ---\n",
    "    for i, c in enumerate(data.get(\"criteria\", []), 1):\n",
    "        if \"criterion\" not in c:\n",
    "            c[\"criterion\"] = (\n",
    "                c.get(\"criteria_name\")\n",
    "                or c.get(\"name\")\n",
    "                or f\"Criterion {i}\"\n",
    "            )\n",
    "        try:\n",
    "            c[\"score\"] = float(c.get(\"score\", 0))\n",
    "        except:\n",
    "            c[\"score\"] = 0.0\n",
    "        c[\"score\"] = max(0.0, min(5.0, c[\"score\"]))\n",
    "        if \"comments\" not in c:\n",
    "            c[\"comments\"] = \"\"\n",
    "\n",
    "    # --- Parse rubric weights dynamically ---\n",
    "    weights = {}\n",
    "    for line in rubric_text.splitlines():\n",
    "        parts = [p.strip() for p in line.split(\"|\")]\n",
    "        if len(parts) >= 3:\n",
    "            crit = parts[0].split(\"(\")[0].strip()\n",
    "            try:\n",
    "                w = float(parts[1])\n",
    "                if 0 <= w <= 1.0:\n",
    "                    weights[crit] = w\n",
    "            except:\n",
    "                pass\n",
    "    if not weights and data.get(\"criteria\"):\n",
    "        eq = 1.0 / len(data[\"criteria\"])\n",
    "        for c in data[\"criteria\"]:\n",
    "            base = c[\"criterion\"].split(\"(\")[0].strip()\n",
    "            weights[base] = eq\n",
    "\n",
    "    # --- Compute weighted total (no strict penalty) ---\n",
    "    total_weight, weighted_sum = 0.0, 0.0\n",
    "    for c in data.get(\"criteria\", []):\n",
    "        base = c[\"criterion\"].split(\"(\")[0].strip()\n",
    "        w = weights.get(base, 1.0 / len(data[\"criteria\"]))\n",
    "        total_weight += w\n",
    "        weighted_sum += (c[\"score\"] / 5.0) * w\n",
    "\n",
    "    final_score = round((weighted_sum / total_weight) * max_score, 2) if total_weight else 0.0\n",
    "\n",
    "    # --- Identify unsupported claims using embeddings ---\n",
    "    unsupported = []\n",
    "    answer_sents = split_sentences(answer)\n",
    "    retr_texts = [r[\"content\"] for r in retrieved]\n",
    "    retr_emb = embedder.encode(retr_texts, convert_to_tensor=True)\n",
    "    sent_emb = embedder.encode(answer_sents, convert_to_tensor=True)\n",
    "    for i, e in enumerate(sent_emb):\n",
    "        max_sim = float(util.cos_sim(e, retr_emb).max().cpu().item())\n",
    "        if max_sim < sim_threshold:\n",
    "            unsupported.append(answer_sents[i])\n",
    "\n",
    "    # --- Retrieve 'correct answer' snippet from notes ---\n",
    "    q_emb = embedder.encode(question, convert_to_tensor=True)\n",
    "    sims = util.cos_sim(q_emb, retr_emb)[0].cpu().tolist()\n",
    "    best_idx = int(max(range(len(sims)), key=lambda i: sims[i]))\n",
    "    best_snippet = retrieved[best_idx][\"content\"].strip()\n",
    "    source_name = retrieved[best_idx][\"source\"]\n",
    "\n",
    "    # --- Compose final clean JSON output ---\n",
    "    output = {\n",
    "        \"criteria\": [\n",
    "            {\n",
    "                \"criterion\": c[\"criterion\"],\n",
    "                \"score\": c[\"score\"],\n",
    "                \"comments\": c[\"comments\"],\n",
    "            }\n",
    "            for c in data.get(\"criteria\", [])\n",
    "        ],\n",
    "        \"unsupported_claims\": unsupported,\n",
    "        \"final_score\": min(max_score, round(final_score, 2)),\n",
    "        \"max_score\": max_score,\n",
    "        \"feedback_summary\": data.get(\n",
    "            \"feedback_summary\",\n",
    "            \"Evaluation based on provided notes and rubric. Unsupported statements noted.\"\n",
    "        ),\n",
    "        \"correct_answer\": {\n",
    "            \"source\": source_name,\n",
    "            \"content\": best_snippet[:700] + (\"...\" if len(best_snippet) > 700 else \"\")\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"criteria\": [\n",
      "    {\n",
      "      \"criterion\": \"Critical Analysis (understanding of course materials)\",\n",
      "      \"score\": 0.0,\n",
      "      \"comments\": \"The response does not engage with the course materials or address the question regarding the centrality of land to the Dakota people's religion and cultural identity.\"\n",
      "    },\n",
      "    {\n",
      "      \"criterion\": \"Academic and Scholarly Presentation\",\n",
      "      \"score\": 0.0,\n",
      "      \"comments\": \"The response lacks clarity and is not related to the question, failing to communicate any ideas effectively.\"\n",
      "    },\n",
      "    {\n",
      "      \"criterion\": \"Portrays Insight (follows instructional questions)\",\n",
      "      \"score\": 0.0,\n",
      "      \"comments\": \"There is no engagement with the instructional questions or course material, resulting in an unacceptable response.\"\n",
      "    }\n",
      "  ],\n",
      "  \"unsupported_claims\": [\n",
      "    \"my name is devendran\"\n",
      "  ],\n",
      "  \"final_score\": 0.0,\n",
      "  \"max_score\": 50,\n",
      "  \"feedback_summary\": \"The response does not address the question or utilize course materials, resulting in a failing grade. Significant improvement is needed in understanding and articulating the relationship between land and Dakota identity.\",\n",
      "  \"correct_answer\": {\n",
      "    \"source\": \"notes.pdf\",\n",
      "    \"content\": \"Dakota, the land remembers.\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "question = \"Why is land considered central to the religion and cultural identity of the Dakota people?\"\n",
    "answer = \"my name is devendran\"\n",
    "result = grade_answer(question, answer, max_score=50, top_k=3, sim_threshold=0.5)\n",
    "final_result = json.dumps(result, indent=2)\n",
    "print(final_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: withpi in /Users/deva/.pyenv/versions/3.11.9/lib/python3.11/site-packages (1.78.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/deva/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from withpi) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/deva/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from withpi) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/deva/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from withpi) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/deva/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from withpi) (2.11.9)\n",
      "Requirement already satisfied: sniffio in /Users/deva/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from withpi) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /Users/deva/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from withpi) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/deva/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from anyio<5,>=3.5.0->withpi) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/deva/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from httpx<1,>=0.23.0->withpi) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/deva/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from httpx<1,>=0.23.0->withpi) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/deva/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->withpi) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/deva/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->withpi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/deva/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->withpi) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/deva/.pyenv/versions/3.11.9/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->withpi) (0.4.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install withpi\n",
    "\n",
    "from withpi import PiClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_spec = [\n",
    "    # --- Output Quality & Correctness ---\n",
    "    {\"question\": \"Does the response follow the specified JSON schema (criteria, scores, comments, etc.)?\"},\n",
    "    {\"question\": \"Is the 'final_score' correctly scaled according to the rubric and max_score?\"},\n",
    "    {\"question\": \"Are all rubric criteria evaluated and present in the output?\"},\n",
    "    {\"question\": \"Do the per-criterion scores and comments align logically with the rubric description?\"},\n",
    "    {\"question\": \"Does the response summarize the student‚Äôs strengths and weaknesses accurately?\"},\n",
    "\n",
    "    # --- Relevance & Grounding ---\n",
    "    {\"question\": \"Is the evaluation grounded in the reference notes, not external knowledge?\"},\n",
    "    {\"question\": \"Are unsupported claims correctly identified and listed?\"},\n",
    "    {\"question\": \"Does the grader avoid giving credit to off-topic or irrelevant content?\"},\n",
    "    {\"question\": \"Does the grader correctly recognize content that directly appears in the notes?\"},\n",
    "\n",
    "    # --- Tone & Feedback Style ---\n",
    "    {\"question\": \"Is the feedback written in a professional, academic tone?\"},\n",
    "    {\"question\": \"Are the comments constructive and informative rather than generic?\"},\n",
    "    {\"question\": \"Is the feedback balanced ‚Äî neither too lenient nor excessively harsh?\"},\n",
    "\n",
    "    # # --- Strictness Level Compliance ---\n",
    "    # {\"question\": \"Does the evaluation reflect the specified strictness level (1‚Äì5)?\"},\n",
    "    # {\"question\": \"If strictness is high (‚â•4), are unsupported or vague answers penalized appropriately?\"},\n",
    "    # {\"question\": \"If strictness is low (‚â§2), is the grader more lenient toward minor missing details?\"},\n",
    "    # {\"question\": \"Does the strictness behavior match the expected tone in the prompt instructions?\"},\n",
    "\n",
    "    # --- Overall Response Validity ---\n",
    "    {\"question\": \"Does the grading fulfill all requirements stated in the prompt?\"},\n",
    "    {\"question\": \"Does the overall result seem consistent, fair, and reliable for the given input?\"},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Score: 0.605\n",
      "Question Scores: {'Are all rubric criteria evaluated and present in the output?': 0.6289, 'Are the comments constructive and informative rather than generic?': 0.6211, 'Are unsupported claims correctly identified and listed?': 0.5078, 'Do the per-criterion scores and comments align logically with the rubric description?': 0.7461, 'Does the grader avoid giving credit to off-topic or irrelevant content?': 0.5234, 'Does the grader correctly recognize content that directly appears in the notes?': 0.5859, 'Does the grading fulfill all requirements stated in the prompt?': 0.6133, 'Does the overall result seem consistent, fair, and reliable for the given input?': 0.582, 'Does the response follow the specified JSON schema (criteria, scores, comments, etc.)?': 0.6719, 'Does the response summarize the student‚Äôs strengths and weaknesses accurately?': 0.6094, \"Is the 'final_score' correctly scaled according to the rubric and max_score?\": 0.4102, 'Is the evaluation grounded in the reference notes, not external knowledge?': 0.5547, 'Is the feedback balanced ‚Äî neither too lenient nor excessively harsh?': 0.7422, 'Is the feedback written in a professional, academic tone?': 0.7852}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"WITHPI_API_KEY\"] = \"sk_d24f2baee781461b85d70c40fe4eedbd\"\n",
    "\n",
    "query = f\"{question} {answer}\"\n",
    "retrieved = retrieve_context(query, top_k=3)\n",
    "context = prepare_indexed_context(retrieved)\n",
    "\n",
    "    # --- Build prompt for the LLM ---\n",
    "myPrompt = build_prompt(\n",
    "        question=question,\n",
    "        answer=answer,\n",
    "        context=context,\n",
    "        rubric_text=rubric_text,\n",
    "        max_score=50,\n",
    ")\n",
    "\n",
    "pi = PiClient()\n",
    "scores = pi.scoring_system.score(\n",
    "  llm_input= myPrompt,\n",
    "  llm_output=final_result,\n",
    "  scoring_spec=scoring_spec\n",
    ")\n",
    "print('Total Score:', scores.total_score)\n",
    "print('Question Scores:', scores.question_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
